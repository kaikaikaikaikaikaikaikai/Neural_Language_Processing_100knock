{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NjMZ4EhSToTM"
      },
      "source": [
        "# 第9章: RNNとCNN\n",
        "*深層学習フレームワークを用い，再帰型ニューラルネットワーク（RNN）や畳み込みニューラルネットワーク（CNN）を実装します*"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### データの前処理(記号と数字の削除)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "#文字列(str)を受け取って記号と数字をすべて削除する関数\n",
        "import re\n",
        "def remove_symbols(text):\n",
        "    text = re.sub(r'[^a-zA-Z\\s]',r'',text).lower()\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/kai/100knock-crash-2023/trainee_kai/chapter09\n",
            "422937 ./data/NewsAggregatorDataset/newsCorpora.csv\n"
          ]
        }
      ],
      "source": [
        "!pwd\n",
        "!wc -l ./data/NewsAggregatorDataset/newsCorpora.csv\n",
        "# 読込時のエラー回避のためダブルクォーテーションをシングルクォーテーションに置換\n",
        "!sed -e 's/\"/'\\''/g' ./data/NewsAggregatorDataset/newsCorpora.csv > ./work/newsCorpora_re.csv\n",
        "\n",
        "#pandasのDataFrameに格納\n",
        "import pandas as pd\n",
        "df = pd.read_csv('./work/newsCorpora_re.csv', header=None, sep='\\t', names=['TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP']) #dataframe型に格納する\n",
        "df = df[(df['PUBLISHER'] == 'Reuters') | (df['PUBLISHER'] == 'Huffington Post') | (df['PUBLISHER'] == 'Businessweek') | (df['PUBLISHER'] == 'Contactmusic.com') | (df['PUBLISHER'] == 'Daily Mail')] #特定のpublisherの記事を抽出\n",
        "# df = df.sample(frac=1,random_state=0) #sampleはランダムにサンプリングするメソッド　fracで指定した割合を母集団から取ってくる、１にするとすべて取ってくる、つまりすべてランダムに並べ替える\n",
        "\n",
        "#記号と数字の削除\n",
        "df['TITLE'] = df['TITLE'].apply(lambda x:remove_symbols(x))\n",
        "\n",
        "#train_test_splitを使って分割\n",
        "from sklearn.model_selection import train_test_split\n",
        "train, valid_test = train_test_split(df,test_size = 0.2)\n",
        "valid, test = train_test_split(valid_test,test_size = 0.5)\n",
        "\n",
        "train = train.reset_index()\n",
        "valid = valid.reset_index()\n",
        "test = test.reset_index()\n",
        "\n",
        "#データの保存\n",
        "train['TITLE'].to_csv('train.txt',sep='\\t',index=False,header=False)#locとつけないとエラーになる、完全には理解できてない\n",
        "valid['TITLE'].to_csv('valid.txt',sep='\\t',index=False,header=False)#複数要素(title,category)で抽出するときにはリストにして渡す\n",
        "test['TITLE'].to_csv('test.txt',sep='\\t',index=False,header=False)#カテゴリ名と記事見出しのタブ区切り形式なのでsep=\\t\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaFViOS5TsYG"
      },
      "source": [
        "## 80 ID番号への変換"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "#テキストファイルから１行ずつトークン化して単語の出現回数を数える\n",
        "words = defaultdict(int)\n",
        "with open('/home/kai/100knock-crash-2023/trainee_kai/chapter09/train.txt','r') as f:\n",
        "    for line in f:\n",
        "        tokens = word_tokenize(line)\n",
        "        for token in tokens:\n",
        "            #出現回数を数えていく。初登場であれば１が入る\n",
        "            words[token] += 1\n",
        "\n",
        "#出現頻度の高い順に並び替え\n",
        "words_sorted = sorted(words.items(), key = lambda x: x[1], reverse=True)\n",
        "\n",
        "#出現頻度の高い順にIDを付与、出現回数が１のものにはID'0'を付与\n",
        "word_ID_dict = {word[0]:ID+1 for ID,word in enumerate(words_sorted) if word[1]>1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[11, 3749, 1192, 4, 142, 4002, 2394, 1918, 2398, 1542, 19]\n"
          ]
        }
      ],
      "source": [
        "# 与えられた単語列に対して，ID番号の列を返す関数\n",
        "def word2id(words):\n",
        "    return [word_ID_dict.get(word.lower(),0) for word in words.split()]\n",
        "\n",
        "# 確認\n",
        "print(word2id('A Brief History of American Apparels Dov Charney Allegedly Doing  Up'))  "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qUce1lnwWLhY"
      },
      "source": [
        "## 81 RNNによる予測"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### クラスの定義"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "V_8N0JbQsDnT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# パラメータの設定\n",
        "# VOCAB_SIZE = len(set(word_ID_dict.values())) + 1  # 単語の種類数＋パディングの１ ダミーIDのために単語IDが1つ余分に消費されるため，宣言時の語彙サイズも1つ大きくする必要がある\n",
        "# EMB_SIZE = 300\n",
        "# PADDING_IDX = len(set(word_ID_dict.values()))\n",
        "# OUTPUT_SIZE = 4\n",
        "# HIDDEN_SIZE = 50\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self,vocab_size,emb_size,num_layers=1,padding_idx=0,output_size=4,hidden_size=50):\n",
        "        '''\n",
        "        vocab_size: 入力単語の種類数\n",
        "        emb_size: 埋め込みベクトルの次元数\n",
        "        num_layers: RNNの層の数\n",
        "        padding_idx: パディングのインデックス\n",
        "        output_size: ラベルの種類。カテゴリ４種類\n",
        "        hidden_size: 隠れ状態ベクトルの次元数\n",
        "        '''\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        # nn.Embedding(埋め込みに使う辞書の大きさ、埋め込みベクトルの次元):単語埋め込みをする\n",
        "        # (系列長) -> (系列長, 埋め込み次元)\n",
        "        self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=padding_idx)\n",
        "        # emb_size : The number of expected features in the input x\n",
        "        # hidden_size : The number of features in the hidden state h\n",
        "        # nonlinearity : 活性化関数 tanhかrelu\n",
        "        # batch_first : True -> output is (batch,seq,feature)\n",
        "        self.rnn = nn.RNN(emb_size, hidden_size, nonlinearity='tanh', batch_first=True)\n",
        "        # in_features : 入力次元数\n",
        "        # out_features :　出力次元数\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.batch_size = x.size()[0]\n",
        "        hidden = self.init_hidden(x.device)  # h0のゼロベクトルを作成\n",
        "        emb = self.emb(x)\n",
        "        # emb.size() = (batch_size, seq_len, emb_size)\n",
        "        out, hidden = self.rnn(emb, hidden)\n",
        "        # out.size() = (batch_size, seq_len, hidden_size)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        # out.size() = (batch_size, output_size)\n",
        "        return out\n",
        "\n",
        "    def init_hidden(self, device):\n",
        "        hidden = torch.zeros(1, self.batch_size, self.hidden_size, device=device)\n",
        "        return hidden\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CreateDataset(Dataset):\n",
        "    def __init__(self, X, y, tokenizer):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):  # len(Dataset)で返す値を指定\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, index):  # Dataset[index]で返す値を指定\n",
        "        text = self.X[index]\n",
        "        inputs = self.tokenizer(text)\n",
        "\n",
        "        return {\n",
        "        'inputs': torch.tensor(inputs, dtype=torch.int64),\n",
        "        'labels': torch.tensor(self.y[index], dtype=torch.int64)\n",
        "        }"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### データセットの作成"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "len(Dataset): 10684\n",
            "Dataset[index]:\n",
            "  inputs: tensor([  65, 2453, 1143,  601,    1, 2177,  146,  715,  107,  882,  622])\n",
            "  labels: 2\n",
            "tensor([ 266,  881,  266,  881,   14, 2176,  245])\n",
            "tensor([  65, 2453, 1143,  601,    1, 2177,  146,  715,  107,  882,  622])\n",
            "tensor([ 193,   17,  177,    1,  748,   62,    7,   39, 1621,   16,  983])\n",
            "tensor([   0,   18,  458,    7,  154,   18,    3, 1959,  458, 1144])\n",
            "tensor([ 425,    1,  199, 1960,  749,   98, 2178,  188,   29,  387,  194])\n",
            "tensor([ 267,  275,   28, 3690,  174,    8,  550,  113,    0,  148])\n",
            "tensor([1784, 1961, 3691,    3, 3175,   13, 1211,   91,    2, 1377])\n",
            "tensor([ 573,    4, 5656,    3,  145, 1622, 2454, 1065,    1,   11, 2455,    7,\n",
            "         438, 2772])\n",
            "tensor([1785, 1623,  276, 1962, 1145,   10, 4476, 1486, 2456,  159,   13])\n",
            "tensor([ 104,  459, 5657,   11,  749,  357,    2,  149,    1,  574, 4477, 4478])\n"
          ]
        }
      ],
      "source": [
        "# ラベルベクトルの作成\n",
        "category_dict = {'b': 0, 't': 1, 'e':2, 'm':3}\n",
        "y_train = train['CATEGORY'].map(lambda x: category_dict[x]).values\n",
        "y_valid = valid['CATEGORY'].map(lambda x: category_dict[x]).values\n",
        "y_test = test['CATEGORY'].map(lambda x: category_dict[x]).values\n",
        "\n",
        "# Datasetの作成\n",
        "dataset_train = CreateDataset(train['TITLE'], y_train, word2id)\n",
        "dataset_valid = CreateDataset(valid['TITLE'], y_valid, word2id)\n",
        "dataset_test = CreateDataset(test['TITLE'], y_test, word2id)\n",
        "\n",
        "print(f'len(Dataset): {len(dataset_train)}')\n",
        "print('Dataset[index]:')\n",
        "for var in dataset_train[1]:\n",
        "    print(f'  {var}: {dataset_train[1][var]}') \n",
        "    \n",
        "# パラメータの設定\n",
        "VOCAB_SIZE = len(set(word_ID_dict.values())) + 1  # 単語の種類数＋パディングの１\n",
        "EMB_SIZE = 300\n",
        "NUM_LAYERS = 1\n",
        "PADDING_IDX = len(set(word_ID_dict.values()))\n",
        "OUTPUT_SIZE = 4\n",
        "HIDDEN_SIZE = 50\n",
        "\n",
        "# モデルの定義\n",
        "model = RNN(VOCAB_SIZE, EMB_SIZE, NUM_LAYERS, PADDING_IDX, OUTPUT_SIZE, HIDDEN_SIZE)\n",
        "\n",
        "# 先頭10件の予測値取得\n",
        "for i in range(10):\n",
        "    X = dataset_train[i]['inputs']\n",
        "    print(X)\n",
        "    #print(torch.softmax(model(X.unsqueeze(0)), dim=-1))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_8sj3v0qWOK1"
      },
      "source": [
        "## 82 確率的勾配降下法による学習"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkai-satou-r8\u001b[0m (\u001b[33mkaikaikaikai\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.15.4 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/kai/100knock-crash-2023/trainee_kai/chapter09/wandb/run-20230703_102426-q56nnc0b</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/kaikaikaikai/nlp100knock-ch08/runs/q56nnc0b' target=\"_blank\">q82</a></strong> to <a href='https://wandb.ai/kaikaikaikai/nlp100knock-ch08' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/kaikaikaikai/nlp100knock-ch08' target=\"_blank\">https://wandb.ai/kaikaikaikai/nlp100knock-ch08</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/kaikaikaikai/nlp100knock-ch08/runs/q56nnc0b' target=\"_blank\">https://wandb.ai/kaikaikaikai/nlp100knock-ch08/runs/q56nnc0b</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# W&B の初期設定\n",
        "import wandb\n",
        "import numpy as np\n",
        "# 保存したいハイパーパラメータを指定\n",
        "config_dict = {\n",
        "        # \"input_dim\": train['TITLE'].shape[1],\n",
        "        # \"output_dim\": len(np.unique(y_train)),\n",
        "        \"lr\": 0.1,\n",
        "        \"epoch\": 100,\n",
        "        \"optimizer\": \"SGD\",\n",
        "        \"loss\": \"CrossEntropyLoss\",\n",
        "        \"metric\": \"accuracy\",\n",
        "    }\n",
        "# W&B の実行を初期化\n",
        "wandb.init(\n",
        "    project=\"nlp100knock-ch08\",\n",
        "    name='q82',\n",
        "    config=config_dict\n",
        "    )\n",
        "# 保存したハイパーパラメータを取得(dict形式)\n",
        "config = wandb.config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "NSqctKZbsET5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 【1/10】,train_loss: 0.317773833017505, train_acc:0.884032197678772, valid_loss: 0.8183775551981602, valid_acc:0.7470059880239521\n",
            "Epoch 【2/10】,train_loss: 0.28510128084897846, train_acc:0.8989142643204793, valid_loss: 0.8185837631677928, valid_acc:0.7612275449101796\n",
            "Epoch 【3/10】,train_loss: 0.33225998961571895, train_acc:0.8849681767128417, valid_loss: 0.844965943890485, valid_acc:0.7447604790419161\n",
            "Epoch 【4/10】,train_loss: 0.3239056474150063, train_acc:0.8829090228378884, valid_loss: 0.7651861743899486, valid_acc:0.7574850299401198\n",
            "Epoch 【5/10】,train_loss: 0.2960176357231746, train_acc:0.8931111943092475, valid_loss: 0.8657488579580213, valid_acc:0.7597305389221557\n",
            "Epoch 【6/10】,train_loss: 0.2673523836671166, train_acc:0.9025645825533508, valid_loss: 0.8517741279124146, valid_acc:0.7724550898203593\n",
            "Epoch 【7/10】,train_loss: 0.2950545200405892, train_acc:0.8933919880194684, valid_loss: 0.8547314057305345, valid_acc:0.7514970059880239\n",
            "Epoch 【8/10】,train_loss: 0.3042068553754022, train_acc:0.8916136278547361, valid_loss: 0.9147244414764264, valid_acc:0.7215568862275449\n",
            "Epoch 【9/10】,train_loss: 0.2967804493160027, train_acc:0.8947959565705729, valid_loss: 0.8388817828432586, valid_acc:0.7440119760479041\n",
            "Epoch 【10/10】,train_loss: 0.2688136715551212, train_acc:0.9084612504679895, valid_loss: 0.891079970761886, valid_acc:0.749251497005988\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>▁▅▂▁▄▆▄▃▄█</td></tr><tr><td>train_loss</td><td>▆▃█▇▄▁▄▅▄▁</td></tr><tr><td>valid_acc</td><td>▅▆▄▆▆█▅▁▄▅</td></tr><tr><td>valid_loss</td><td>▃▃▅▁▆▅▅█▄▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>0.90846</td></tr><tr><td>train_loss</td><td>0.26881</td></tr><tr><td>valid_acc</td><td>0.74925</td></tr><tr><td>valid_loss</td><td>0.89108</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">q82</strong> at: <a href='https://wandb.ai/kaikaikaikai/nlp100knock-ch08/runs/bgnisemx' target=\"_blank\">https://wandb.ai/kaikaikaikai/nlp100knock-ch08/runs/bgnisemx</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230702_154023-bgnisemx/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#パディングの処理をしていないはず?なのにちゃんと動いているっぽい理由が知りたい\n",
        "#バッチサイズが1だといけちゃうっぽい\n",
        "from torch import optim\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "\n",
        "#学習率\n",
        "lr = 0.01\n",
        "#エポック数\n",
        "num_epochs = 10\n",
        "history = np.zeros((0,3))\n",
        "#損失関数\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "#最適化関数 SGD: stochastic gradient descent 確率的勾配降下法\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "base_epochs = len(history)\n",
        "#デバイスを切り替えてる、なくても動きはする？\n",
        "device = 'cpu'\n",
        "model = model.to(device)\n",
        "\n",
        "#wandbに関するところ\n",
        "config = wandb.config\n",
        "wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
        "\n",
        "batch_size = 1\n",
        "\n",
        "train_loader = DataLoader(dataset_train,batch_size=batch_size, shuffle=True)\n",
        "valid_loader = DataLoader(dataset_valid,batch_size=batch_size, shuffle=False)\n",
        "\n",
        "#ここから学習の本体\n",
        "for epoch in range(base_epochs, base_epochs+num_epochs):\n",
        "    start_time = time.time()\n",
        "    train_loss, train_acc = 0,0\n",
        "    valid_loss, valid_acc = 0,0\n",
        "\n",
        "    '''訓練フェーズ'''\n",
        "    model.train()\n",
        "    count = 0\n",
        "    for data in train_loader:\n",
        "        \n",
        "        inputs = data['inputs']#.to(torch.float32)\n",
        "        labels = data['labels']#.to(torch.int64)\n",
        "        count += len(labels)\n",
        "        #デバイスの割り当て\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        #勾配の初期化\n",
        "        optimizer.zero_grad()\n",
        "        #予測計算\n",
        "        outputs = model(inputs)\n",
        "        #損失計算\n",
        "        loss = criterion(outputs, labels)\n",
        "        train_loss += loss.item()\n",
        "        #勾配計算\n",
        "        loss.backward()\n",
        "        #パラメータ修正\n",
        "        optimizer.step()\n",
        "        #予測値算出\n",
        "        predicted = torch.max(outputs, 1)[1]\n",
        "        #正解した数を数えて、長さで割る\n",
        "        train_acc += (predicted == labels).sum().item()\n",
        "        #損失と精度の計算\n",
        "        avg_train_loss = train_loss / count\n",
        "        avg_train_acc = train_acc / count\n",
        "        \n",
        "    model.eval()\n",
        "    count = 0\n",
        "    with torch.no_grad():\n",
        "        for data in valid_loader:\n",
        "            #型を揃えないとエラー\n",
        "            inputs = data['inputs']#.to(torch.float32)\n",
        "            labels = data['labels']#.to(torch.int64)\n",
        "            count += len(labels)\n",
        "            #デバイスの割り当て\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            #勾配の初期化\n",
        "            optimizer.zero_grad()\n",
        "            #予測計算\n",
        "            outputs = model(inputs)\n",
        "            #損失計算\n",
        "            loss = criterion(outputs, labels)\n",
        "            valid_loss += loss.item()\n",
        "            #予測値算出\n",
        "            predicted = torch.max(outputs, 1)[1]\n",
        "            #正解した数を数えて、長さで割る\n",
        "            valid_acc += (predicted == labels).sum().item()\n",
        "            #損失と精度の計算\n",
        "            avg_valid_loss = valid_loss / count\n",
        "            avg_valid_acc = valid_acc / count\n",
        "    \n",
        "    print (f'Epoch 【{epoch+1}/{num_epochs+base_epochs}】,train_loss: {avg_train_loss}, train_acc:{avg_train_acc}, valid_loss: {avg_valid_loss}, valid_acc:{avg_valid_acc}')\n",
        "    wandb.log({\"train_loss\":avg_train_loss,\"train_acc\":avg_train_acc,\"valid_loss\":avg_valid_loss,\"valid_acc\":avg_valid_acc})\n",
        "wandb.finish()\n",
        "    "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uJPaSr31WRkn"
      },
      "source": [
        "## 83 ミニバッチ化・GPU上での学習"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "IOw7ZDdAsE_W"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from torch import optim\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def fit(\n",
        "        model,\n",
        "        optimizer,\n",
        "        criterion,\n",
        "        num_epochs,\n",
        "        train_loader,\n",
        "        valid_loader,\n",
        "        history,# : NDArray[Shape[\"*, 5\"], Float64]\n",
        "        wandb=wandb,\n",
        "        device=\"cpu\"\n",
        "        ) : #-> NDArray[Shape[\"*, 5\"], Float64]\n",
        "    \"\"\"\n",
        "    docstring:\n",
        "    the function for training neural network models\n",
        "    \n",
        "    Args:\n",
        "        model: Model instances defined in pytorch. This function will train this model.\n",
        "        optimizer: Optimization function.\n",
        "        criterion: Loss funciton.\n",
        "        num_epochs: Number of epochs.\n",
        "        train_loader: Dataloader for trianing data.\n",
        "        valid_loader: Dataloader for valid data.\n",
        "        history: NDArray containing the loss and accuracy for each epoch.\n",
        "                np.array[<number of epochs>,<loss against training data>,<accuracy against training data>]\n",
        "        wandb: Inistialized wandb instance.\n",
        "        device: Haedware(CPU or GPU) to run the training.\n",
        "        \n",
        "    Return:\n",
        "        history: NDArray containing the loss and accuracy for each epoch\n",
        "    \"\"\"\n",
        "\n",
        "    \n",
        "    base_epochs = len(history)\n",
        "    model = model.to(device)\n",
        "\n",
        "    #wandbに関するところ\n",
        "    config = wandb.config\n",
        "    wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
        "\n",
        "    #ここから学習の本体\n",
        "    for epoch in range(base_epochs, base_epochs+num_epochs):\n",
        "        start_time = time.time()\n",
        "        train_loss, train_acc = 0,0\n",
        "        valid_loss, valid_acc = 0,0\n",
        "\n",
        "        '''訓練フェーズ'''\n",
        "        model.train()\n",
        "        count = 0\n",
        "        for data in train_loader:\n",
        "            \n",
        "            inputs = data['inputs']#.to(torch.float32)\n",
        "            labels = data['labels']#.to(torch.int64)\n",
        "            count += len(labels)\n",
        "            #デバイスの割り当て\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            #勾配の初期化\n",
        "            optimizer.zero_grad()\n",
        "            #予測計算\n",
        "            outputs = model(inputs)\n",
        "            #損失計算\n",
        "            loss = criterion(outputs, labels)\n",
        "            train_loss += loss.item()\n",
        "            #勾配計算\n",
        "            loss.backward()\n",
        "            #パラメータ修正\n",
        "            optimizer.step()\n",
        "            #予測値算出\n",
        "            predicted = torch.max(outputs, 1)[1]\n",
        "            #正解した数を数えて、長さで割る\n",
        "            train_acc += (predicted == labels).sum().item()\n",
        "            #損失と精度の計算\n",
        "            avg_train_loss = train_loss / count\n",
        "            avg_train_acc = train_acc / count\n",
        "            \n",
        "        model.eval()\n",
        "        count = 0\n",
        "        with torch.no_grad():\n",
        "            for data in valid_loader:\n",
        "                #型を揃えないとエラー\n",
        "                inputs = data['inputs']#.to(torch.float32)\n",
        "                labels = data['labels']#.to(torch.int64)\n",
        "                count += len(labels)\n",
        "                #デバイスの割り当て\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                #勾配の初期化\n",
        "                optimizer.zero_grad()\n",
        "                #予測計算\n",
        "                outputs = model(inputs)\n",
        "                #損失計算\n",
        "                loss = criterion(outputs, labels)\n",
        "                valid_loss += loss.item()\n",
        "                #予測値算出\n",
        "                predicted = torch.max(outputs, 1)[1]\n",
        "                #正解した数を数えて、長さで割る\n",
        "                valid_acc += (predicted == labels).sum().item()\n",
        "                #損失と精度の計算\n",
        "                avg_valid_loss = valid_loss / count\n",
        "                avg_valid_acc = valid_acc / count\n",
        "        \n",
        "        #10回ごとに表示\n",
        "        if ((epoch+1)%10 == 0):\n",
        "            print (f'Epoch 【{epoch+1}/{num_epochs+base_epochs}】,train_loss: {train_loss}, train_acc:{train_acc}')\n",
        "            wandb.log({\"train_loss\":avg_train_loss,\"train_acc\":avg_train_acc,\"valid_loss\":avg_valid_loss,\"valid_acc\":avg_valid_acc})\n",
        "    wandb.finish()\n",
        "    return history\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:6om2w8xs) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">q83</strong> at: <a href='https://wandb.ai/kaikaikaikai/nlp100knock-ch08/runs/6om2w8xs' target=\"_blank\">https://wandb.ai/kaikaikaikai/nlp100knock-ch08/runs/6om2w8xs</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230703_102438-6om2w8xs/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:6om2w8xs). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.15.4 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/kai/100knock-crash-2023/trainee_kai/chapter09/wandb/run-20230703_105820-t1jycyet</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/kaikaikaikai/nlp100knock-ch08/runs/t1jycyet' target=\"_blank\">q83</a></strong> to <a href='https://wandb.ai/kaikaikaikai/nlp100knock-ch08' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/kaikaikaikai/nlp100knock-ch08' target=\"_blank\">https://wandb.ai/kaikaikaikai/nlp100knock-ch08</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/kaikaikaikai/nlp100knock-ch08/runs/t1jycyet' target=\"_blank\">https://wandb.ai/kaikaikaikai/nlp100knock-ch08/runs/t1jycyet</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# W&B の初期設定\n",
        "import wandb\n",
        "# 保存したいハイパーパラメータを指定\n",
        "config_dict = {\n",
        "        \"lr\": 0.1,\n",
        "        \"epoch\": 100,\n",
        "        \"optimizer\": \"SGD\",\n",
        "        \"loss\": \"CrossEntropyLoss\",\n",
        "        \"metric\": \"accuracy\",\n",
        "    }\n",
        "# W&B の実行を初期化\n",
        "wandb.init(\n",
        "    project=\"nlp100knock-ch08\",\n",
        "    name='q83',\n",
        "    config=config_dict\n",
        "    )\n",
        "# 保存したハイパーパラメータを取得(dict形式)\n",
        "config = wandb.config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'inputs': tensor([ 266,  881,  266,  881,   14, 2176,  245]), 'labels': tensor(2)}\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'Tensor' object has no attribute 'append'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-ca089a3152f8>\u001b[0m in \u001b[0;36m<cell line: 43>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0mvalid_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-65-ca089a3152f8>\u001b[0m in \u001b[0;36mpadding\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mxs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'inputs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'append'"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "lr=0.01\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "#損失関数\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#エポック数\n",
        "num_epochs = 10\n",
        "\n",
        "#バッチサイズ\n",
        "batch_size = 1\n",
        "\n",
        "print(dataset_train[0])\n",
        "\n",
        "#ミニバッチを取り出して長さを揃える関数\n",
        "def My_collate_func(batch):\n",
        "    xs, ys = [], []\n",
        "    print(batch)\n",
        "    print(batch[0]['inputs'])\n",
        "    x = batch[0]['inputs']\n",
        "    y = batch[0]['labels']\n",
        "    xs.append(torch.LongTensor(x))\n",
        "    ys.append(torch.LongTensor(y))\n",
        "    #データ長を揃える処理\n",
        "    xs = pad_sequence(xs, batch_first=True)\n",
        "    print(xs)\n",
        "    ys = pad_sequence(ys, batch_first=True, padding_value=-1.0)\n",
        "    print(ys)\n",
        "    return xs, ys\n",
        "\n",
        "def padding(dataset):\n",
        "    xs, ys = [], []\n",
        "    for i in range(len(dataset)):\n",
        "        xs.append(dataset[i]['inputs'])\n",
        "        ys.append(dataset[i]['labels'])\n",
        "        xs = pad_sequence(xs, batch_first=True)\n",
        "    return Dataset(xs, ys)\n",
        "\n",
        "train_loader = DataLoader(padding(dataset_train),batch_size=batch_size, shuffle=True)\n",
        "valid_loader = DataLoader(padding(dataset_valid),batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(padding(dataset_test),batch_size=batch_size, shuffle=False)\n",
        "\n",
        "for data in train_loader:\n",
        "    print(data)\n",
        "\n",
        "#     inputs = data['inputs']#.to(torch.float32)\n",
        "#     labels = data['labels']#.to(torch.int64)\n",
        "#     print(inputs)\n",
        "#history = np.zeros((0,3))\n",
        "\n",
        "#最適化関数 SGD: stochastic gradient descent 確率的勾配降下法\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "history = fit(model,optimizer,criterion,num_epochs,train_loader,valid_loader,history,wandb,device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# class CreateDataset(Dataset):\n",
        "#     def __init__(self, X, y, tokenizer):\n",
        "#         self.X = X\n",
        "#         self.y = y\n",
        "#         self.tokenizer = tokenizer\n",
        "\n",
        "#     def __len__(self):  \n",
        "#         return len(self.y)\n",
        "\n",
        "#     def __getitem__(self, index):  \n",
        "#         text = self.X[index]\n",
        "#         inputs = self.tokenizer(text)\n",
        "\n",
        "#         return {\n",
        "#         'inputs': torch.tensor(inputs, dtype=torch.int64),\n",
        "#         'labels': torch.tensor(self.y[index], dtype=torch.int64)\n",
        "#         }\n",
        "        \n",
        "# dataset_train = CreateDataset(train['TITLE'], y_train, word2id)\n",
        "# dataset_valid = CreateDataset(valid['TITLE'], y_valid, word2id)\n",
        "\n",
        "# def My_collate_func(batch):\n",
        "#     xs, ys = [], []\n",
        "#     for x,y in batch:\n",
        "#         xs.append(torch.LongTensor(x))\n",
        "#         ys.append(torch.LongTensor(y))\n",
        "#     #データ長を揃える処理\n",
        "#     xs = pad_sequence(xs, batch_first=True)\n",
        "#     print(xs)\n",
        "#     ys = pad_sequence(ys, batch_first=True, padding_value=-1.0)\n",
        "#     print(ys)\n",
        "#     return xs, ys\n",
        "\n",
        "# train_loader = DataLoader(dataset_train,batch_size=batch_size, shuffle=True, collate_fn=My_collate_func)\n",
        "# valid_loader = DataLoader(dataset_valid,batch_size=batch_size, shuffle=False, collate_fn=My_collate_func)\n",
        "\n",
        "# for data in train_loader:\n",
        "#     print(data)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iCKpxYPXWUan"
      },
      "source": [
        "## 84 単語ベクトルの導入"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCDC_JAJsFcq"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "K-eM15b0WW5-"
      },
      "source": [
        "## 85 双方向RNN・多層化"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EANUZjqXsGMC"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TXriUvevWZxG"
      },
      "source": [
        "## 86 畳み込みニューラルネットワーク (CNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-afI9yG5sG4Z"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4ttZhF9HWcpu"
      },
      "source": [
        "## 87 確率的勾配降下法によるCNNの学習"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EH9EyUVvsHjx"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jbHDdaUcWgXV"
      },
      "source": [
        "## 88 パラメータチューニング"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHIEbE1ssIOm"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PN0A_9dVWjoo"
      },
      "source": [
        "## 89 事前学習済み言語モデルからの転移学習"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQd-NnA8sIx0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
